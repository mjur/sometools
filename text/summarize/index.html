<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Text Summarizer | AI-Powered Text Summarization | SomeTools</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta name="description" content="Summarize text using AI. Runs entirely in your browser with WebLLM.">
  <link rel="canonical" href="https://sometools.io/text/summarize">
  <link rel="stylesheet" href="/css/base.css">
  <link rel="stylesheet" href="/css/tool.css">
  <link rel="preload" href="/css/base.css" as="style">
  <link rel="icon" href="/favicon.ico" sizes="any">
  <link rel="icon" href="/assets/download-16x16.ico" sizes="16x16" type="image/x-icon">
  <link rel="icon" href="/assets/download-32x32.ico" sizes="32x32" type="image/x-icon">
  <link rel="icon" href="/assets/download-48x48.ico" sizes="48x48" type="image/x-icon">
  <link rel="icon" href="/assets/download-64x64.ico" sizes="64x64" type="image/x-icon">
  <link rel="icon" href="/assets/download-128x128.ico" sizes="128x128" type="image/x-icon">
  <link rel="icon" href="/assets/download-256x256.ico" sizes="256x256" type="image/x-icon">
  <link rel="apple-touch-icon" href="/assets/download-256x256.ico">
  <link rel="manifest" href="/manifest.webmanifest">
  <script type="module" src="/js/analytics.js"></script>
  <meta property="og:title" content="Text Summarizer">
  <meta property="og:description" content="Summarize text using AI. Runs entirely in your browser with WebLLM.">
  <style>
    details.info-box summary {
      list-style: none;
    }
    details.info-box summary::-webkit-details-marker {
      display: none;
    }
    details.info-box summary::marker {
      display: none;
    }
    details.info-box summary:hover {
      opacity: 0.8;
    }
  </style>
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "FAQPage",
    "mainEntity": [
      {
        "@type": "Question",
        "name": "How does the text summarizer work?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "The text summarizer uses WebLLM, a browser-based AI model, to generate summaries from text. The model runs entirely in your browser."
        }
      },
      {
        "@type": "Question",
        "name": "Do I need to download a model?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "Yes, the AI model needs to be downloaded and cached in your browser. This happens automatically on first use, or you can download it manually using the 'Download & Cache Model' button. Some models are large and may require increasing your browser's cache size."
        }
      }
    ]
  }
  </script>
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      {
        "@type": "ListItem",
        "position": 1,
        "name": "Home",
        "item": "https://sometools.io/"
      },
      {
        "@type": "ListItem",
        "position": 2,
        "name": "Text Summarizer",
        "item": "https://sometools.io/text/summarize"
      }
    ]
  }
  </script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to main content</a>
  <header>
    <a href="/">
      <h1>SomeTools</h1>
    </a>
    <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
      <span id="theme-icon">üåô</span>
    </button>
  </header>
  
  <nav aria-label="Breadcrumb">
    <a href="/">Home</a>
    <a href="/text/summarize">Text Summarizer</a>
  </nav>
  
  <main id="main">
    <h1>Text Summarizer</h1>
    <p>Summarize text using AI. Powered by WebLLM - runs entirely in your browser.</p>
    
    <details class="info-box" style="padding: 1rem; background: var(--bg-elev); border: 1px solid var(--border); border-radius: 6px; margin-bottom: 1.5rem;">
      <summary style="cursor: pointer; font-weight: 600; list-style: none; user-select: none;">
        <span style="display: inline-flex; align-items: center; gap: 0.5rem;">
          <span>‚ö†Ô∏è Important: WebLLM Requirements</span>
          <span style="font-size: 0.875rem; color: var(--muted); font-weight: normal;">(click to expand)</span>
        </span>
      </summary>
      <div style="margin-top: 1rem;">
        <p><strong>WebLLM requires WebGPU support in your browser.</strong></p>
        <ul style="margin: 0.5rem 0; padding-left: 1.5rem;">
          <li><strong>WebGPU Support:</strong> Required for model execution. Check support at <a href="https://webgpureport.org/" target="_blank">webgpureport.org</a></li>
          <li><strong>Browser Requirements:</strong> Chrome 113+, Edge 113+, or Safari 18+ (with WebGPU enabled)</li>
          <li><strong>GPU:</strong> A compatible GPU (dedicated or integrated) is recommended for best performance</li>
          <li><strong>AI Models:</strong> Large files (ranging from ~300MB to several GB) that need to be downloaded</li>
          <li>Models are cached in your browser's IndexedDB for faster subsequent use</li>
          <li>Some models may exceed your browser's default cache limit</li>
        </ul>
        <details style="margin-top: 0.5rem;">
          <summary style="cursor: pointer; font-weight: bold;">How to increase browser cache size</summary>
          <ul style="margin-top: 0.5rem; padding-left: 1.5rem;">
            <li><strong>Chrome/Edge:</strong> Settings ‚Üí Site settings ‚Üí Storage ‚Üí Increase quota</li>
            <li><strong>Firefox:</strong> about:config ‚Üí search "dom.storage.max_quota" ‚Üí increase value</li>
            <li><strong>Safari:</strong> Preferences ‚Üí Advanced ‚Üí Show Develop menu ‚Üí Develop ‚Üí Empty Caches (then increase in Develop menu)</li>
          </ul>
        </details>
        <details style="margin-top: 0.5rem;">
          <summary style="cursor: pointer; font-weight: bold;">Troubleshooting WebLLM Issues</summary>
          <ul style="margin-top: 0.5rem; padding-left: 1.5rem;">
            <li><strong>WebGPU errors:</strong> WebLLM requires WebGPU support. Use Chrome 113+, Edge 113+, or Safari 18+. Check support at <a href="https://webgpureport.org/" target="_blank">webgpureport.org</a></li>
            <li><strong>Enable WebGPU in Chrome:</strong> Go to chrome://flags/#enable-unsafe-webgpu and enable it, then restart Chrome</li>
            <li><strong>Shader compilation errors:</strong> Usually indicates GPU compatibility issues. Try updating GPU drivers, using a different browser, or selecting a smaller model</li>
            <li><strong>GPU requirements:</strong> A compatible GPU (dedicated or integrated) is required. Some older or integrated GPUs may not support all WebGPU features</li>
            <li><strong>Update GPU drivers:</strong> Outdated drivers can cause shader errors. Update to the latest drivers from your GPU manufacturer</li>
            <li><strong>Try smaller models:</strong> If you encounter errors, try the smallest models first (Qwen2.5-0.5B or Qwen2-0.5B)</li>
            <li><strong>If you see "Failed to load WebLLM":</strong> Check the browser console (F12) for detailed errors</li>
            <li><strong>CORS errors:</strong> Make sure you're accessing via http://localhost:8000 or https:// (not file://)</li>
            <li><strong>Network errors:</strong> Ensure you have internet connection for initial library download</li>
            <li><strong>Model download errors:</strong> Check your browser's storage quota and internet connection</li>
          </ul>
        </details>
      </div>
    </details>
    
    <section class="tool">
      <div class="pane">
        <label for="model-select">AI Model</label>
        <select id="model-select">
          <optgroup label="Smallest Models (Recommended for Quick Start)">
            <option value="Qwen2.5-0.5B-Instruct-q4f16_1-MLC" selected>Qwen2.5 0.5B Instruct (~300MB)</option>
            <option value="Qwen2-0.5B-Instruct-q4f16_1-MLC">Qwen2 0.5B Instruct (~300MB)</option>
            <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama 3.2 1B Instruct (~600MB)</option>
            <option value="TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC">TinyLlama 1.1B Chat v1.0 (~600MB)</option>
            <option value="SmolLM2-360M-Instruct-q4f16_1-MLC">SmolLM2 360M Instruct (~200MB)</option>
          </optgroup>
          
          <optgroup label="Small Models (Good Balance)">
            <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 1.5B Instruct (~850MB)</option>
            <option value="Qwen2-1.5B-Instruct-q4f16_1-MLC">Qwen2 1.5B Instruct (~850MB)</option>
            <option value="Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B Instruct (~1.8GB)</option>
            <option value="Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes 3 Llama 3.2 3B (~1.8GB)</option>
            <option value="Phi-3.5-mini-instruct-q4f16_1-MLC">Phi-3.5 Mini Instruct (~2.3GB)</option>
            <option value="Phi-3-mini-4k-instruct-q4f16_1-MLC">Phi-3 Mini 4K Instruct (~2.3GB)</option>
          </optgroup>
          
          <optgroup label="Medium Models (Better Quality)">
            <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC">Llama 3.1 8B Instruct (~4.6GB)</option>
            <option value="Llama-3-8B-Instruct-q4f16_1-MLC">Llama 3 8B Instruct (~4.6GB)</option>
            <option value="Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC">Hermes 2 Pro Llama 3 8B (~4.6GB)</option>
            <option value="Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes 3 Llama 3.1 8B (~4.6GB)</option>
            <option value="Qwen2.5-7B-Instruct-q4f16_1-MLC">Qwen2.5 7B Instruct (~3.8GB)</option>
            <option value="Qwen2-7B-Instruct-q4f16_1-MLC">Qwen2 7B Instruct (~3.8GB)</option>
            <option value="Mistral-7B-Instruct-v0.3-q4f16_1-MLC">Mistral 7B Instruct v0.3 (~3.8GB)</option>
            <option value="Hermes-2-Pro-Mistral-7B-q4f16_1-MLC">Hermes 2 Pro Mistral 7B (~3.8GB)</option>
          </optgroup>
          
          <optgroup label="Large Models (Best Quality, Requires More Storage)">
            <option value="Llama-3-70B-Instruct-q3f16_1-MLC">Llama 3 70B Instruct (~39GB)</option>
            <option value="Llama-3.1-70B-Instruct-q3f16_1-MLC">Llama 3.1 70B Instruct (~39GB)</option>
          </optgroup>
          
          <optgroup label="Specialized Models">
            <option value="Qwen2.5-Math-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 Math 1.5B Instruct (~850MB)</option>
            <option value="Qwen2-Math-7B-Instruct-q4f16_1-MLC">Qwen2 Math 7B Instruct (~3.8GB)</option>
            <option value="Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC">Qwen2.5 Coder 7B Instruct (~3.8GB)</option>
            <option value="WizardMath-7B-V1.1-q4f16_1-MLC">WizardMath 7B V1.1 (~3.8GB)</option>
            <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 Distill Qwen 7B (~3.8GB)</option>
            <option value="DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC">DeepSeek R1 Distill Llama 8B (~4.6GB)</option>
          </optgroup>
        </select>
        <div class="actions" style="margin-top: 0.5rem; flex-wrap: wrap; gap: 0.5rem;">
          <button id="download-model" class="primary">Download & Cache Model</button>
          <button id="check-model">Check Model Status</button>
          <button id="clear-model" class="secondary">Clear Selected Model</button>
        </div>
        <div id="model-status" style="margin-top: 0.5rem; padding: 0.5rem; background: var(--bg-elev); border-radius: 4px; font-size: 0.875rem; color: var(--muted); white-space: pre-wrap; min-height: 3rem; max-height: 20rem; overflow-y: auto;"></div>
        <div id="model-list" style="margin-top: 0.5rem; padding: 0.5rem; background: var(--bg-elev); border-radius: 4px; font-size: 0.8rem; color: var(--muted); max-height: 10rem; overflow-y: auto;"></div>
        
        <label for="input-text" style="margin-top: 1.5rem;">Text to Summarize</label>
        <div class="drop-zone" id="drop-zone" style="border: 2px dashed var(--border); border-radius: 6px; padding: 1rem; text-align: center; cursor: pointer; margin-bottom: 0.5rem; background: var(--bg-elev);">
          <p style="margin: 0; color: var(--muted);">Drag and drop a text file here, or click to select</p>
          <p class="text-sm text-muted" style="margin: 0.25rem 0 0 0; font-size: 0.875rem;">Supported: .txt, .md, .markdown, and other text files</p>
          <input type="file" id="file-input" accept=".txt,.md,.markdown,text/*" style="display: none;">
        </div>
        <textarea id="input-text" placeholder="Paste or type the text you want to summarize here..." style="min-height: 200px;"></textarea>
        
        <div class="actions" style="margin-top: 0.5rem;">
          <button id="summarize" class="primary">Summarize</button>
          <button id="clear">Clear</button>
        </div>
      </div>
      
      <div class="pane">
        <label for="summary">Summary</label>
        <textarea id="summary" readonly placeholder="Summary will appear here..." style="min-height: 200px;"></textarea>
        
        <div class="actions" style="margin-top: 0.5rem;">
          <button id="copy-summary">Copy Summary</button>
          <button id="download-summary">Download Summary</button>
        </div>
      </div>
    </section>
    
    <section class="faq" aria-labelledby="faq-h">
      <h2 id="faq-h">FAQ</h2>
      <details>
        <summary>How does the text summarizer work?</summary>
        <p>The text summarizer uses WebLLM, a browser-based AI model, to generate summaries from text. The model runs entirely in your browser.</p>
      </details>
      <details>
        <summary>Do I need to download a model?</summary>
        <p>Yes, the AI model needs to be downloaded and cached in your browser. This happens automatically on first use, or you can download it manually using the "Download & Cache Model" button. Some models are large and may require increasing your browser's cache size.</p>
      </details>
      <details>
        <summary>Which model should I use?</summary>
        <p>For most use cases, Qwen2.5 0.5B or Phi-3 Mini 4K are recommended as they provide a good balance between size and performance. Larger models like Llama 3 8B provide better quality summaries but require more storage space.</p>
      </details>
      <details>
        <summary>Can I use this offline?</summary>
        <p>Once the model is downloaded and cached, you can use the summarizer offline. The model runs entirely in your browser without requiring an internet connection after the initial download.</p>
      </details>
      <details>
        <summary>Is my data sent to a server?</summary>
        <p>No. All processing happens entirely in your browser.</p>
      </details>
    </section>
  </main>
  
  <script type="module" src="/js/header.js"></script>
  <!-- Load bundled WebLLM (shared by regex generator and chatbot) -->
  <script src="https://cdn.jsdelivr.net/npm/jszip@3.10.1/dist/jszip.min.js"></script>
  <script type="module">
    try {
      await import('/js/tools/bundled/webllm-bundle.js');
    } catch (e) {
      console.log('Bundled WebLLM not found. Run "npm install && npm run build" to create it.');
    }
  </script>
  <script type="module" src="/js/tools/text-summarize.js"></script>
  <script type="module">
    import { initModelCacheWidget } from '/js/utils/model-cache-widget.js';
    initModelCacheWidget();
  </script>
  <script type="module">
    import { initNotesWidget } from '/js/utils/notes-widget.js';
    initNotesWidget();
  </script>
</body>
</html>

