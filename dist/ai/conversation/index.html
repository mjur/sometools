<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>AI Conversation | SomeTools</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta name="description" content="Watch two AI models converse with each other in your browser using WebLLM. Set the topic and model settings, then watch the conversation unfold.">
  <link rel="canonical" href="https://example.com/ai/conversation">
  <link rel="stylesheet" href="/css/base.css">
  <link rel="stylesheet" href="/css/tool.css">
  <link rel="preload" href="/css/base.css" as="style">
  <link rel="icon" href="/favicon.ico" sizes="any">
  <link rel="icon" href="/assets/download-16x16.ico" sizes="16x16" type="image/x-icon">
  <link rel="icon" href="/assets/download-32x32.ico" sizes="32x32" type="image/x-icon">
  <link rel="icon" href="/assets/download-48x48.ico" sizes="48x48" type="image/x-icon">
  <link rel="icon" href="/assets/download-64x64.ico" sizes="64x64" type="image/x-icon">
  <link rel="icon" href="/assets/download-128x128.ico" sizes="128x128" type="image/x-icon">
  <link rel="icon" href="/assets/download-256x256.ico" sizes="256x256" type="image/x-icon">
  <link rel="apple-touch-icon" href="/assets/download-256x256.ico">
  <link rel="manifest" href="/manifest.webmanifest">
  <script type="module" src="/js/analytics.js"></script>
  <meta property="og:title" content="AI Conversation">
  <meta property="og:description" content="Watch two AI models converse with each other locally in your browser.">
  <style>
    details.info-box summary {
      list-style: none;
    }
    details.info-box summary::-webkit-details-marker {
      display: none;
    }
    details.info-box summary::marker {
      display: none;
    }
    details.info-box summary:hover {
      opacity: 0.8;
    }
    .ai-message {
      margin-bottom: 1rem;
      padding: 0.75rem;
      border-radius: 6px;
      border: 1px solid var(--border);
    }
    .ai-message.ai1 {
      background: linear-gradient(135deg, var(--bg-elev) 0%, rgba(59, 130, 246, 0.1) 100%);
      border-left: 3px solid #3b82f6;
    }
    .ai-message.ai2 {
      background: linear-gradient(135deg, var(--bg-elev) 0%, rgba(16, 185, 129, 0.1) 100%);
      border-left: 3px solid #10b981;
    }
    .ai-message .speaker {
      font-weight: 600;
      margin-bottom: 0.25rem;
      font-size: 0.875rem;
    }
    .ai-message.ai1 .speaker {
      color: #3b82f6;
    }
    .ai-message.ai2 .speaker {
      color: #10b981;
    }
    .ai-message .content {
      white-space: pre-wrap;
      line-height: 1.5;
    }
    #model-selection-pane.collapsed {
      max-height: 60px;
      overflow: hidden;
    }
    #model-selection-content {
      transition: opacity 0.3s ease, max-height 0.3s ease;
      overflow: hidden;
    }
    #model-selection-pane.collapsed #model-selection-content {
      max-height: 0;
      opacity: 0;
      margin: 0;
      padding: 0;
      overflow: hidden;
    }
    #model-selection-toggle {
      cursor: pointer;
      transition: all 0.2s ease;
    }
    #model-selection-toggle:hover {
      background: var(--bg-hover);
    }
    @media (max-width: 1023px) {
      section.tool {
        grid-template-columns: 1fr !important;
      }
    }
  </style>
</head>
<body>
  <a href="#main" class="skip-link">Skip to main content</a>
  <header>
    <a href="/">
      <h1>SomeTools</h1>
    </a>
    <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
      <span id="theme-icon">üåô</span>
    </button>
  </header>

  <nav aria-label="Breadcrumb">
    <a href="/">Home</a>
    <a href="/ai/conversation">AI Conversation</a>
  </nav>

  <main id="main">
    <h1>AI Conversation</h1>
    <p>Watch two AI models have a conversation with each other, running entirely in your browser using WebLLM. Set a topic to start the conversation, configure model settings, and observe the exchange.</p>

    <details class="info-box" style="padding: 1rem; background: var(--bg-elev); border: 1px solid var(--border); border-radius: 6px; margin-bottom: 1.5rem;">
      <summary style="cursor: pointer; font-weight: 600; list-style: none; user-select: none;">
        <span style="display: inline-flex; align-items: center; gap: 0.5rem;">
          <span>‚ö†Ô∏è WebLLM Requirements</span>
          <span style="font-size: 0.875rem; color: var(--muted); font-weight: normal;">(click to expand)</span>
        </span>
      </summary>
      <div style="margin-top: 1rem;">
        <p><strong>WebLLM requires WebGPU support in your browser.</strong></p>
        <ul style="margin: 0.5rem 0; padding-left: 1.5rem;">
          <li><strong>WebGPU Support:</strong> Required for model execution. Check support at <a href="https://webgpureport.org/" target="_blank" rel="noreferrer">webgpureport.org</a></li>
          <li><strong>Browser Requirements:</strong> Chrome 113+, Edge 113+, or Safari 18+ (with WebGPU enabled)</li>
          <li><strong>Models:</strong> Large files that are downloaded once and cached in your browser's IndexedDB</li>
          <li><strong>Performance:</strong> Running two models simultaneously requires significant GPU memory. Smaller models are recommended.</li>
          <li>You can use the same model for both AIs or choose different models for each.</li>
        </ul>
      </div>
    </details>

    <section class="tool" style="position: relative; display: flex; flex-direction: column; gap: 1.5rem;">
      <div class="pane" id="model-selection-pane" style="position: relative;">
        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 0.5rem;">
          <label style="margin: 0;">Model Configuration</label>
          <button id="model-selection-toggle" class="secondary" style="font-size: 1.2rem; padding: 0.3rem 0.6rem; min-width: 32px;">‚àí</button>
        </div>
        <div id="model-selection-content">
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin-bottom: 1rem;">
            <div>
              <label for="ai1-model-select">AI 1 Model</label>
              <select id="ai1-model-select">
                <optgroup label="Smallest Models (Recommended for Quick Start)">
                  <option value="Qwen2.5-0.5B-Instruct-q4f16_1-MLC" selected>Qwen2.5 0.5B Instruct (~300MB)</option>
                  <option value="Qwen2-0.5B-Instruct-q4f16_1-MLC">Qwen2 0.5B Instruct (~300MB)</option>
                  <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama 3.2 1B Instruct (~600MB)</option>
                  <option value="TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC">TinyLlama 1.1B Chat v1.0 (~600MB)</option>
                  <option value="SmolLM2-360M-Instruct-q4f16_1-MLC">SmolLM2 360M Instruct (~200MB)</option>
                </optgroup>

                <optgroup label="Small Models (Good Balance)">
                  <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 1.5B Instruct (~850MB)</option>
                  <option value="Qwen2-1.5B-Instruct-q4f16_1-MLC">Qwen2 1.5B Instruct (~850MB)</option>
                  <option value="Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B Instruct (~1.8GB)</option>
                  <option value="Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes 3 Llama 3.2 3B (~1.8GB)</option>
                  <option value="Phi-3.5-mini-instruct-q4f16_1-MLC">Phi-3.5 Mini Instruct (~2.3GB)</option>
                  <option value="Phi-3-mini-4k-instruct-q4f16_1-MLC">Phi-3 Mini 4K Instruct (~2.3GB)</option>
                </optgroup>

                <optgroup label="Medium Models (Better Quality)">
                  <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC">Llama 3.1 8B Instruct (~4.6GB)</option>
                  <option value="Llama-3-8B-Instruct-q4f16_1-MLC">Llama 3 8B Instruct (~4.6GB)</option>
                  <option value="Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC">Hermes 2 Pro Llama 3 8B (~4.6GB)</option>
                  <option value="Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes 3 Llama 3.1 8B (~4.6GB)</option>
                  <option value="Qwen2.5-7B-Instruct-q4f16_1-MLC">Qwen2.5 7B Instruct (~3.8GB)</option>
                  <option value="Qwen2-7B-Instruct-q4f16_1-MLC">Qwen2 7B Instruct (~3.8GB)</option>
                  <option value="Mistral-7B-Instruct-v0.3-q4f16_1-MLC">Mistral 7B Instruct v0.3 (~3.8GB)</option>
                  <option value="Hermes-2-Pro-Mistral-7B-q4f16_1-MLC">Hermes 2 Pro Mistral 7B (~3.8GB)</option>
                </optgroup>

                <optgroup label="Specialized Models">
                  <option value="Qwen2.5-Math-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 Math 1.5B Instruct (~850MB)</option>
                  <option value="Qwen2-Math-7B-Instruct-q4f16_1-MLC">Qwen2 Math 7B Instruct (~3.8GB)</option>
                  <option value="Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC">Qwen2.5 Coder 7B Instruct (~3.8GB)</option>
                  <option value="WizardMath-7B-V1.1-q4f16_1-MLC">WizardMath 7B V1.1 (~3.8GB)</option>
                  <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 Distill Qwen 7B (~3.8GB)</option>
                  <option value="DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC">DeepSeek R1 Distill Llama 8B (~4.6GB)</option>
                </optgroup>
              </select>
            </div>
            <div>
              <label for="ai2-model-select">AI 2 Model</label>
              <select id="ai2-model-select">
                <optgroup label="Smallest Models (Recommended for Quick Start)">
                  <option value="Qwen2.5-0.5B-Instruct-q4f16_1-MLC">Qwen2.5 0.5B Instruct (~300MB)</option>
                  <option value="Qwen2-0.5B-Instruct-q4f16_1-MLC">Qwen2 0.5B Instruct (~300MB)</option>
                  <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC" selected>Llama 3.2 1B Instruct (~600MB)</option>
                  <option value="TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC">TinyLlama 1.1B Chat v1.0 (~600MB)</option>
                  <option value="SmolLM2-360M-Instruct-q4f16_1-MLC">SmolLM2 360M Instruct (~200MB)</option>
                </optgroup>

                <optgroup label="Small Models (Good Balance)">
                  <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 1.5B Instruct (~850MB)</option>
                  <option value="Qwen2-1.5B-Instruct-q4f16_1-MLC">Qwen2 1.5B Instruct (~850MB)</option>
                  <option value="Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B Instruct (~1.8GB)</option>
                  <option value="Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes 3 Llama 3.2 3B (~1.8GB)</option>
                  <option value="Phi-3.5-mini-instruct-q4f16_1-MLC">Phi-3.5 Mini Instruct (~2.3GB)</option>
                  <option value="Phi-3-mini-4k-instruct-q4f16_1-MLC">Phi-3 Mini 4K Instruct (~2.3GB)</option>
                </optgroup>

                <optgroup label="Medium Models (Better Quality)">
                  <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC">Llama 3.1 8B Instruct (~4.6GB)</option>
                  <option value="Llama-3-8B-Instruct-q4f16_1-MLC">Llama 3 8B Instruct (~4.6GB)</option>
                  <option value="Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC">Hermes 2 Pro Llama 3 8B (~4.6GB)</option>
                  <option value="Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes 3 Llama 3.1 8B (~4.6GB)</option>
                  <option value="Qwen2.5-7B-Instruct-q4f16_1-MLC">Qwen2.5 7B Instruct (~3.8GB)</option>
                  <option value="Qwen2-7B-Instruct-q4f16_1-MLC">Qwen2 7B Instruct (~3.8GB)</option>
                  <option value="Mistral-7B-Instruct-v0.3-q4f16_1-MLC">Mistral 7B Instruct v0.3 (~3.8GB)</option>
                  <option value="Hermes-2-Pro-Mistral-7B-q4f16_1-MLC">Hermes 2 Pro Mistral 7B (~3.8GB)</option>
                </optgroup>

                <optgroup label="Specialized Models">
                  <option value="Qwen2.5-Math-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 Math 1.5B Instruct (~850MB)</option>
                  <option value="Qwen2-Math-7B-Instruct-q4f16_1-MLC">Qwen2 Math 7B Instruct (~3.8GB)</option>
                  <option value="Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC">Qwen2.5 Coder 7B Instruct (~3.8GB)</option>
                  <option value="WizardMath-7B-V1.1-q4f16_1-MLC">WizardMath 7B V1.1 (~3.8GB)</option>
                  <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 Distill Qwen 7B (~3.8GB)</option>
                  <option value="DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC">DeepSeek R1 Distill Llama 8B (~4.6GB)</option>
                </optgroup>
              </select>
            </div>
          </div>

          <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 1rem; margin-bottom: 1rem;">
            <div>
              <label for="temperature-input">Temperature</label>
              <input type="number" id="temperature-input" min="0" max="2" step="0.1" value="0.7" style="width: 100%;">
              <small style="color: var(--muted);">Creativity level (0.0-2.0)</small>
            </div>
            <div>
              <label for="max-tokens-input">Max Tokens</label>
              <input type="number" id="max-tokens-input" min="50" max="2048" step="50" value="256" style="width: 100%;">
              <small style="color: var(--muted);">Response length</small>
            </div>
            <div>
              <label for="max-turns-input">Max Turns</label>
              <input type="number" id="max-turns-input" min="1" max="50" step="1" value="10" style="width: 100%;">
              <small style="color: var(--muted);">Conversation length</small>
            </div>
          </div>

          <div class="actions" style="margin-top: 0.5rem; flex-wrap: wrap; gap: 0.5rem;">
            <button id="download-models-btn" class="primary">Download & Cache Models</button>
            <button id="check-models-btn">Check Models Status</button>
          </div>

          <div id="models-status" style="margin-top: 0.5rem; padding: 0.5rem; background: var(--bg-elev); border-radius: 4px; font-size: 0.875rem; color: var(--muted); white-space: pre-wrap; min-height: 3rem;"></div>
        </div>
      </div>

      <div class="pane" style="display: flex; flex-direction: column; height: 100%; flex: 1;">
        <label for="conversation-topic">Conversation Topic</label>
        <textarea id="conversation-topic" rows="2" placeholder="Enter a topic or question to start the conversation... (e.g., 'Discuss the future of artificial intelligence')" style="margin-bottom: 1rem; resize: vertical;"></textarea>
        
        <div class="actions" style="margin-bottom: 1rem;">
          <button id="start-conversation-btn" class="primary">Start Conversation</button>
          <button id="stop-conversation-btn" class="secondary" disabled>Stop Conversation</button>
          <button id="clear-conversation-btn" class="secondary">Clear</button>
        </div>

        <label for="conversation-log">Conversation</label>
        <div id="conversation-log" style="flex: 1; min-height: 400px; border: 1px solid var(--border); border-radius: 6px; background: var(--bg-elev); padding: 0.75rem; overflow-y: auto; font-size: 0.9rem;">
          <p class="text-sm text-muted">Select models, enter a conversation topic, then click "Start Conversation" to watch two AIs discuss the topic.</p>
        </div>
      </div>
    </section>

    <section class="faq" aria-labelledby="faq-h">
      <h2 id="faq-h">FAQ</h2>
      <details>
        <summary>How does this work?</summary>
        <p>This tool loads two AI models in your browser and has them converse with each other. The first AI responds to your initial prompt, then the second AI responds to the first AI's message, and so on. All processing happens locally in your browser using WebLLM.</p>
      </details>
      <details>
        <summary>Can I use the same model for both AIs?</summary>
        <p>Yes! You can use the same model for both AIs. The tool will load the model once and reuse it for both conversation participants. This is more memory-efficient than loading two different models.</p>
      </details>
      <details>
        <summary>Why does it take so long to start?</summary>
        <p>If the models aren't already cached, they need to be downloaded first. Smaller models (~300MB-1GB) download quickly, while larger models can take several minutes. Once cached, they load much faster.</p>
      </details>
      <details>
        <summary>What are good settings for a conversation?</summary>
        <p>Start with smaller models like Qwen2.5 0.5B or Llama 3.2 1B. Set temperature to 0.7-0.9 for more varied responses. Keep max tokens around 256 for faster responses. Start with 5-10 turns to see how the conversation flows.</p>
      </details>
      <details>
        <summary>Is my data sent to a server?</summary>
        <p>No. All models run entirely in your browser. No prompts or responses are sent to any server.</p>
      </details>
    </section>
  </main>

  <script type="module" src="/js/header.js"></script>
  <!-- Load bundled WebLLM -->
  <script type="module">
    try {
      await import('/js/tools/bundled/webllm-bundle.js');
    } catch (e) {
      console.log('Bundled WebLLM not found. Run "npm install && npm run build" to create it.');
    }
  </script>
  <script type="module" src="/js/tools/webllm-ai-conversation.js"></script>
  <script type="module">
    import { initNotesWidget } from '/js/utils/notes-widget.js';
    initNotesWidget();
  </script>
</body>
</html>

