<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>WebLLM Chatbot | BunchOfTools</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta name="description" content="Chat with local AI models in your browser using WebLLM. Download, cache, and clear models directly from this page.">
  <link rel="canonical" href="https://example.com/ai/chat">
  <link rel="stylesheet" href="/css/base.css">
  <link rel="stylesheet" href="/css/tool.css">
  <link rel="preload" href="/css/base.css" as="style">
  <link rel="icon" href="/favicon.ico">
  <link rel="manifest" href="/manifest.webmanifest">
  <script type="module" src="/js/analytics.js"></script>
  <meta property="og:title" content="WebLLM Chatbot">
  <meta property="og:description" content="Chat with WebLLM models locally in your browser.">
</head>
<body>
  <a href="#main" class="skip-link">Skip to main content</a>
  <header>
    <a href="/">
      <h1>BunchOfTools</h1>
    </a>
    <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
      <span id="theme-icon">üåô</span>
    </button>
  </header>

  <nav aria-label="Breadcrumb">
    <a href="/">Home</a>
    <a href="/ai/chat">WebLLM Chatbot</a>
  </nav>

  <main id="main">
    <h1>WebLLM Chatbot</h1>
    <p>Chat with AI models that run entirely in your browser using WebLLM. Download models once, cache them locally, and clear them any time.</p>

    <div class="info-box" style="padding: 1rem; background: var(--bg-elev); border: 1px solid var(--border); border-radius: 6px; margin-bottom: 1.5rem;">
      <h3 style="margin-top: 0;">‚ö†Ô∏è WebLLM Requirements</h3>
      <p><strong>WebLLM requires WebGPU support in your browser.</strong></p>
      <ul style="margin: 0.5rem 0; padding-left: 1.5rem;">
        <li><strong>WebGPU Support:</strong> Required for model execution. Check support at <a href="https://webgpureport.org/" target="_blank" rel="noreferrer">webgpureport.org</a></li>
        <li><strong>Browser Requirements:</strong> Chrome 113+, Edge 113+, or Safari 18+ (with WebGPU enabled)</li>
        <li><strong>Models:</strong> Large files that are downloaded once and cached in your browser's IndexedDB</li>
        <li>You can clear individual models or all WebLLM cache from this page.</li>
      </ul>
    </div>

    <section class="tool">
      <div class="pane">
        <label for="chat-model-select">AI Model</label>
        <select id="chat-model-select">
          <optgroup label="Smallest Models (Recommended for Quick Start)">
            <option value="Qwen2.5-0.5B-Instruct-q4f16_1-MLC" selected>Qwen2.5 0.5B Instruct (~300MB)</option>
            <option value="Qwen2-0.5B-Instruct-q4f16_1-MLC">Qwen2 0.5B Instruct (~300MB)</option>
            <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama 3.2 1B Instruct (~600MB)</option>
            <option value="TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC">TinyLlama 1.1B Chat v1.0 (~600MB)</option>
            <option value="SmolLM2-360M-Instruct-q4f16_1-MLC">SmolLM2 360M Instruct (~200MB)</option>
          </optgroup>

          <optgroup label="Small Models (Good Balance)">
            <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 1.5B Instruct (~850MB)</option>
            <option value="Qwen2-1.5B-Instruct-q4f16_1-MLC">Qwen2 1.5B Instruct (~850MB)</option>
            <option value="Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama 3.2 3B Instruct (~1.8GB)</option>
            <option value="Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes 3 Llama 3.2 3B (~1.8GB)</option>
            <option value="Phi-3.5-mini-instruct-q4f16_1-MLC">Phi-3.5 Mini Instruct (~2.3GB)</option>
            <option value="Phi-3-mini-4k-instruct-q4f16_1-MLC">Phi-3 Mini 4K Instruct (~2.3GB)</option>
          </optgroup>

          <optgroup label="Medium Models (Better Quality)">
            <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC">Llama 3.1 8B Instruct (~4.6GB)</option>
            <option value="Llama-3-8B-Instruct-q4f16_1-MLC">Llama 3 8B Instruct (~4.6GB)</option>
            <option value="Hermes-2-Pro-Llama-3-8B-q4f16_1-MLC">Hermes 2 Pro Llama 3 8B (~4.6GB)</option>
            <option value="Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes 3 Llama 3.1 8B (~4.6GB)</option>
            <option value="Qwen2.5-7B-Instruct-q4f16_1-MLC">Qwen2.5 7B Instruct (~3.8GB)</option>
            <option value="Qwen2-7B-Instruct-q4f16_1-MLC">Qwen2 7B Instruct (~3.8GB)</option>
            <option value="Mistral-7B-Instruct-v0.3-q4f16_1-MLC">Mistral 7B Instruct v0.3 (~3.8GB)</option>
            <option value="Hermes-2-Pro-Mistral-7B-q4f16_1-MLC">Hermes 2 Pro Mistral 7B (~3.8GB)</option>
          </optgroup>

          <optgroup label="Large Models (Best Quality, Requires More Storage)">
            <option value="Llama-3-70B-Instruct-q3f16_1-MLC">Llama 3 70B Instruct (~39GB)</option>
            <option value="Llama-3.1-70B-Instruct-q3f16_1-MLC">Llama 3.1 70B Instruct (~39GB)</option>
          </optgroup>

          <optgroup label="Specialized Models">
            <option value="Qwen2.5-Math-1.5B-Instruct-q4f16_1-MLC">Qwen2.5 Math 1.5B Instruct (~850MB)</option>
            <option value="Qwen2-Math-7B-Instruct-q4f16_1-MLC">Qwen2 Math 7B Instruct (~3.8GB)</option>
            <option value="Qwen2.5-Coder-7B-Instruct-q4f16_1-MLC">Qwen2.5 Coder 7B Instruct (~3.8GB)</option>
            <option value="WizardMath-7B-V1.1-q4f16_1-MLC">WizardMath 7B V1.1 (~3.8GB)</option>
            <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek R1 Distill Qwen 7B (~3.8GB)</option>
            <option value="DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC">DeepSeek R1 Distill Llama 8B (~4.6GB)</option>
          </optgroup>
        </select>

        <div class="actions" style="margin-top: 0.5rem; flex-wrap: wrap; gap: 0.5rem;">
          <button id="chat-download-model" class="primary">Download & Cache Model</button>
          <button id="chat-check-model">Check Model Status</button>
          <button id="chat-clear-model" class="secondary">Clear Selected Model</button>
        </div>

        <div id="chat-model-status" style="margin-top: 0.5rem; padding: 0.5rem; background: var(--bg-elev); border-radius: 4px; font-size: 0.875rem; color: var(--muted); white-space: pre-wrap; min-height: 3rem; max-height: 20rem; overflow-y: auto;"></div>
        <div id="chat-model-list" style="margin-top: 0.5rem; padding: 0.5rem; background: var(--bg-elev); border-radius: 4px; font-size: 0.8rem; color: var(--muted); max-height: 10rem; overflow-y: auto;"></div>
      </div>

      <div class="pane" style="display: flex; flex-direction: column; height: 100%;">
        <label for="chat-log">Chat</label>
        <div id="chat-log" style="height: 320px; flex: 0 0 320px; border: 1px solid var(--border); border-radius: 6px; background: var(--bg-elev); padding: 0.75rem; overflow-y: auto; font-size: 0.9rem;">
          <p class="text-sm text-muted">Select a model, download it if needed, then start chatting. Your messages and responses stay in your browser.</p>
        </div>
        <div style="margin-top: 0.75rem; display: flex; flex-direction: column; gap: 0.5rem;">
          <textarea id="chat-input" rows="3" placeholder="Ask a question or start a conversation..." style="resize: vertical;"></textarea>
          <div class="actions" style="display: flex; justify-content: space-between; align-items: center; gap: 0.5rem;">
            <div class="text-sm text-muted">Press <kbd>Cmd/Ctrl+Enter</kbd> to send</div>
            <div style="display: flex; gap: 0.5rem;">
              <button id="chat-clear-conversation" class="secondary">Clear conversation</button>
              <button id="chat-send" class="primary">Send</button>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="faq" aria-labelledby="faq-h">
      <h2 id="faq-h">FAQ</h2>
      <details>
        <summary>Where are models stored?</summary>
        <p>WebLLM stores models in your browser's IndexedDB under the <code>webllm</code> database. You can clear individual models or all models using the buttons above.</p>
      </details>
      <details>
        <summary>Can I use this offline?</summary>
        <p>Yes. After a model has been downloaded and cached, you can chat offline. The model runs entirely in your browser.</p>
      </details>
      <details>
        <summary>Is my chat data sent to a server?</summary>
        <p>No. All prompts and responses are processed locally by WebLLM in your browser.</p>
      </details>
    </section>
  </main>

  <script type="module" src="/js/header.js"></script>
  <!-- Load bundled WebLLM (shared with regex generator) -->
  <script type="module">
    try {
      await import('/js/tools/bundled/webllm-bundle.js');
    } catch (e) {
      console.log('Bundled WebLLM not found. Run \"npm install && npm run build\" to create it.');
    }
  </script>
  <script type="module" src="/js/tools/webllm-chat.js"></script>
</body>
</html>


