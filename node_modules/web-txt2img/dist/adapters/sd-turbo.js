import { fetchArrayBufferWithCacheProgress, purgeModelCache } from '../cache.js';
// Minimal adapter scaffold. Actual ONNX pipeline is TBD.
export class SDTurboAdapter {
    constructor() {
        this.id = 'sd-turbo';
        this.loaded = false;
        this.backendUsed = null;
        this.ort = null;
        this.sessions = {};
        this.tokenizerFn = null;
        this.tokenizerProvider = null;
        this.modelBase = 'https://huggingface.co/schmuell/sd-turbo-ort-web/resolve/main';
    }
    checkSupport(c) {
        const backends = [];
        if (c.webgpu)
            backends.push('webgpu');
        // WASM is assumed available
        backends.push('wasm');
        return backends;
    }
    async load(options) {
        const preferred = options.backendPreference;
        const supported = ['webgpu', 'wasm'];
        let chosen = preferred.find((b) => supported.includes(b));
        if (!chosen)
            return { ok: false, reason: 'backend_unavailable', message: 'No viable backend for SD-Turbo' };
        // Resolve model base URL override
        if (options.modelBaseUrl)
            this.modelBase = options.modelBaseUrl;
        if (options.tokenizerProvider)
            this.tokenizerProvider = options.tokenizerProvider;
        // Resolve ORT runtime: injected → dynamic import → global
        try {
            let ort = options.ort ?? null;
            if (!ort) {
                let ortMod = null;
                if (chosen === 'webgpu') {
                    ortMod = await import('onnxruntime-web/webgpu').catch(() => null);
                }
                else {
                    // WASM uses the default entry
                    ortMod = await import('onnxruntime-web').catch(() => null);
                }
                ort = ortMod && (ortMod.default ?? ortMod);
            }
            if (!ort) {
                const gOrt = globalThis.ort; // fallback if app added <script>
                if (gOrt)
                    ort = gOrt;
            }
            if (!ort) {
                return { ok: false, reason: 'internal_error', message: 'onnxruntime-web not available. Install as a dependency or inject via loadModel({ ort }).' };
            }
            this.ort = ort;
        }
        catch (e) {
            return { ok: false, reason: 'internal_error', message: `Failed to load onnxruntime-web: ${e instanceof Error ? e.message : String(e)}` };
        }
        // Placeholder for downloading model assets using Cache Storage
        try {
            options.onProgress?.({ phase: 'loading', message: 'Preparing SD-Turbo model...' });
            this.backendUsed = chosen;
            const ort = this.ort;
            const opt = {
                executionProviders: [chosen],
                enableMemPattern: false,
                enableCpuMemArena: false,
                extra: {
                    session: {
                        disable_prepacking: '1',
                        use_device_allocator_for_initializers: '1',
                        use_ort_model_bytes_directly: '1',
                        use_ort_model_bytes_for_initializers: '1',
                    },
                },
            };
            if (chosen === 'webgpu') {
                opt.preferredOutputLocation = { last_hidden_state: 'gpu-buffer' };
            }
            // Configure WASM env if provided, regardless of EP; ORT may still load WASM helpers
            try {
                if (options.wasmPaths)
                    ort.env.wasm.wasmPaths = options.wasmPaths;
                if (typeof options.wasmNumThreads === 'number')
                    ort.env.wasm.numThreads = options.wasmNumThreads;
                if (typeof options.wasmSimd === 'boolean')
                    ort.env.wasm.simd = options.wasmSimd;
            }
            catch { }
            const models = {
                unet: {
                    url: 'unet/model.onnx', sizeMB: 640,
                    opt: { freeDimensionOverrides: { batch_size: 1, num_channels: 4, height: 64, width: 64, sequence_length: 77 } },
                },
                text_encoder: {
                    url: 'text_encoder/model.onnx', sizeMB: 1700,
                    opt: { freeDimensionOverrides: { batch_size: 1 } },
                },
                vae_decoder: {
                    url: 'vae_decoder/model.onnx', sizeMB: 95,
                    opt: { freeDimensionOverrides: { batch_size: 1, num_channels_latent: 4, height_latent: 64, width_latent: 64 } },
                },
            };
            // compute base URL
            const base = this.modelBase;
            // Fetch and create sessions with progress
            let bytesDownloaded = 0;
            // Use approximate grand total injected from registry (single source of truth)
            const fallbackTotal = Object.values(models).reduce((acc, m) => acc + m.sizeMB * 1024 * 1024, 0);
            const GRAND_APPROX = (typeof options.approxTotalBytes === 'number' ? options.approxTotalBytes : fallbackTotal);
            options.onProgress?.({
                phase: 'loading',
                message: `starting downloads (~${Math.round(GRAND_APPROX / 1024 / 1024)}MB total)...`,
                bytesDownloaded: 0,
                totalBytesExpected: GRAND_APPROX,
                pct: 0,
                accuracy: 'exact',
            });
            for (const key of Object.keys(models)) {
                const model = models[key];
                options.onProgress?.({ phase: 'loading', message: `downloading ${model.url}...`, bytesDownloaded });
                const expectedTotal = model.sizeMB * 1024 * 1024;
                const buf = await fetchArrayBufferWithCacheProgress(`${base}/${model.url}`, this.id, (loaded, total) => {
                    const pct = Math.min(100, Math.round(((bytesDownloaded + loaded) / GRAND_APPROX) * 100));
                    options.onProgress?.({
                        phase: 'loading',
                        message: `downloading ${model.url}...`,
                        pct,
                        bytesDownloaded: bytesDownloaded + loaded,
                        totalBytesExpected: GRAND_APPROX,
                        asset: model.url,
                        accuracy: 'exact',
                    });
                }, expectedTotal);
                bytesDownloaded += buf.byteLength;
                const start = performance.now();
                const sess = await ort.InferenceSession.create(buf, { ...opt, ...model.opt });
                const ms = performance.now() - start;
                options.onProgress?.({
                    phase: 'loading',
                    message: `${model.url} ready in ${ms.toFixed(1)}ms`,
                    bytesDownloaded,
                    totalBytesExpected: GRAND_APPROX,
                    asset: model.url,
                    accuracy: 'exact',
                });
                this.sessions[key] = sess;
            }
            this.loaded = true;
            return { ok: true, backendUsed: chosen, bytesDownloaded };
        }
        catch (e) {
            console.error('[sd-turbo] load error', e);
            return { ok: false, reason: 'internal_error', message: e instanceof Error ? e.message : String(e) };
        }
    }
    isLoaded() {
        return this.loaded;
    }
    async generate(params) {
        if (!this.loaded)
            return { ok: false, reason: 'model_not_loaded', message: 'Call loadModel() first' };
        const { prompt, width = 512, height = 512, signal, onProgress, seed, num_inference_steps = 1 } = params;
        if (!prompt || !prompt.trim())
            return { ok: false, reason: 'unsupported_option', message: 'Prompt is required' };
        if (width !== 512 || height !== 512) {
            return { ok: false, reason: 'unsupported_option', message: 'Only 512x512 is supported in v1' };
        }
        const start = performance.now();
        const ort = this.ort;
        try {
            // Tokenizer (injected or dynamic)
            onProgress?.({ phase: 'tokenizing', pct: 5 });
            if (!this.tokenizerFn) {
                if (this.tokenizerProvider)
                    this.tokenizerFn = await this.tokenizerProvider();
                else
                    this.tokenizerFn = await getTokenizer();
            }
            if (signal?.aborted) {
                onProgress?.({ phase: 'complete', aborted: true, pct: 0 });
                return { ok: false, reason: 'cancelled' };
            }
            const tok = this.tokenizerFn;
            const { input_ids } = await tok(prompt, { padding: true, max_length: 77, truncation: true, return_tensor: false });
            // Text encoder
            onProgress?.({ phase: 'encoding', pct: 15 });
            const ids = Int32Array.from(input_ids);
            let encOut;
            try {
                encOut = await this.sessions.text_encoder.run({ input_ids: new ort.Tensor('int32', ids, [1, ids.length]) });
            }
            catch (e) {
                throw new Error(`text_encoder.run failed: ${e instanceof Error ? e.message : String(e)}`);
            }
            const last_hidden_state = encOut.last_hidden_state ?? encOut;
            if (signal?.aborted) {
                onProgress?.({ phase: 'complete', aborted: true, pct: 0 });
                return { ok: false, reason: 'cancelled' };
            }
            // Latents
            const latent_shape = [1, 4, 64, 64];
            const vae_scaling_factor = 0.18215;
            let latent = new ort.Tensor(randn_latents(latent_shape, 14.6146, seed), latent_shape);
            
            // Generate timestep schedule
            const timesteps = generateTimesteps(num_inference_steps);
            const sigmas = timestepsToSigmas(timesteps);
            
            // Denoising loop
            for (let i = 0; i < num_inference_steps; i++) {
                const step_pct = 20 + Math.floor((i / num_inference_steps) * 70);
                onProgress?.({ phase: 'denoising', pct: step_pct, message: `Step ${i + 1}/${num_inference_steps}` });
                
                if (signal?.aborted) {
                    onProgress?.({ phase: 'complete', aborted: true, pct: 0 });
                    return { ok: false, reason: 'cancelled' };
                }
                
                const sigma = sigmas[i];
                const latent_model_input = scale_model_inputs(ort, latent, sigma);
                const tstep = [BigInt(timesteps[i])];
                
                const feed = {
                    sample: latent_model_input,
                    timestep: new ort.Tensor('int64', tstep, [1]),
                    encoder_hidden_states: last_hidden_state,
                };
                
                let out_sample;
                try {
                    out_sample = await this.sessions.unet.run(feed);
                    out_sample = out_sample.out_sample ?? out_sample;
                }
                catch (e) {
                    throw new Error(`unet.run failed at step ${i}: ${e instanceof Error ? e.message : String(e)}`);
                }
                
                // Scheduler step
                const sigma_next = i < num_inference_steps - 1 ? sigmas[i + 1] : 0;
                latent = schedulerStep(ort, out_sample, latent, sigma, sigma_next, i === num_inference_steps - 1 ? vae_scaling_factor : 1.0);
            }
            
            if (typeof last_hidden_state.dispose === 'function')
                last_hidden_state.dispose();
            
            // VAE decode
            onProgress?.({ phase: 'decoding', pct: 95 });
            if (signal?.aborted) {
                onProgress?.({ phase: 'complete', aborted: true, pct: 0 });
                return { ok: false, reason: 'cancelled' };
            }
            let vaeOut;
            try {
                vaeOut = await this.sessions.vae_decoder.run({ latent_sample: latent });
            }
            catch (e) {
                throw new Error(`vae_decoder.run failed: ${e instanceof Error ? e.message : String(e)}`);
            }
            const sample = vaeOut.sample ?? vaeOut;
            if (signal?.aborted) {
                onProgress?.({ phase: 'complete', aborted: true, pct: 0 });
                return { ok: false, reason: 'cancelled' };
            }
            const blob = await tensorToPngBlob(sample);
            const timeMs = performance.now() - start;
            onProgress?.({ phase: 'complete', pct: 100, timeMs });
            return { ok: true, blob, timeMs };
        }
        catch (e) {
            console.error('[sd-turbo] generate error', e);
            return { ok: false, reason: 'internal_error', message: e instanceof Error ? e.message : String(e) };
        }
    }
    async unload() {
        try {
            // Dispose ORT sessions if available and clear references
            try {
                this.sessions.unet?.release?.();
            }
            catch { }
            try {
                this.sessions.text_encoder?.release?.();
            }
            catch { }
            try {
                this.sessions.vae_decoder?.release?.();
            }
            catch { }
        }
        finally {
            this.sessions = {};
            this.ort = null;
            this.loaded = false;
            this.backendUsed = null;
        }
    }
    async purgeCache() {
        await purgeModelCache(this.id);
    }
}
// Helpers
function mulberry32(seed) {
    let t = seed >>> 0;
    return function () {
        t += 0x6D2B79F5;
        let r = Math.imul(t ^ (t >>> 15), 1 | t);
        r ^= r + Math.imul(r ^ (r >>> 7), 61 | r);
        return ((r ^ (r >>> 14)) >>> 0) / 4294967296;
    };
}
function randn_latents(shape, noise_sigma, seed) {
    const rand = seed !== undefined ? mulberry32(seed) : Math.random;
    function randn() {
        const u = rand();
        const v = rand();
        return Math.sqrt(-2 * Math.log(u)) * Math.cos(2 * Math.PI * v);
    }
    let size = 1;
    for (const s of shape)
        size *= s;
    const data = new Float32Array(size);
    for (let i = 0; i < size; i++)
        data[i] = randn() * noise_sigma;
    return data;
}
function scale_model_inputs(ort, t, sigma) {
    const d_i = t.data;
    const d_o = new Float32Array(d_i.length);
    const divi = Math.sqrt(sigma * sigma + 1);
    for (let i = 0; i < d_i.length; i++)
        d_o[i] = d_i[i] / divi;
    return new ort.Tensor(d_o, t.dims);
}
// Generate timestep schedule (linearly spaced from 999 to 0)
function generateTimesteps(num_steps) {
    if (num_steps === 1) return [999];
    const timesteps = [];
    for (let i = 0; i < num_steps; i++) {
        const t = Math.round(999 - (i * 999 / (num_steps - 1)));
        timesteps.push(Math.max(0, t));
    }
    return timesteps;
}
// Convert timesteps to sigmas
function timestepsToSigmas(timesteps) {
    // Simplified sigma schedule for SD-Turbo
    // Original sigma at t=999 is ~14.6146, at t=0 is 0
    return timesteps.map(t => 14.6146 * (t / 999));
}
// Euler scheduler step for multi-step generation
function schedulerStep(ort, model_output, sample, sigma, sigma_next, scale_factor = 1.0) {
    const d_o = new Float32Array(model_output.data.length);
    const sigma_hat = sigma;
    for (let i = 0; i < model_output.data.length; i++) {
        const pred_original_sample = sample.data[i] - sigma_hat * model_output.data[i];
        const derivative = (sample.data[i] - pred_original_sample) / sigma_hat;
        const dt = sigma_next - sigma_hat;
        d_o[i] = (sample.data[i] + derivative * dt) / scale_factor;
    }
    return new ort.Tensor(d_o, model_output.dims);
}
// Legacy single-step function (kept for compatibility)
function step(ort, model_output, sample, sigma, vae_scaling_factor) {
    return schedulerStep(ort, model_output, sample, sigma, 0, vae_scaling_factor);
}
async function tensorToPngBlob(t) {
    // t: [1, 3, H, W]
    const [n, c, h, w] = t.dims;
    const data = t.data;
    const out = new Uint8ClampedArray(w * h * 4);
    let idx = 0;
    for (let y = 0; y < h; y++) {
        for (let x = 0; x < w; x++) {
            const r = data[0 * h * w + y * w + x];
            const g = data[1 * h * w + y * w + x];
            const b = data[2 * h * w + y * w + x];
            const clamp = (v) => {
                let x = v / 2 + 0.5;
                if (x < 0)
                    x = 0;
                if (x > 1)
                    x = 1;
                return Math.round(x * 255);
            };
            out[idx++] = clamp(r);
            out[idx++] = clamp(g);
            out[idx++] = clamp(b);
            out[idx++] = 255;
        }
    }
    const imageData = new ImageData(out, w, h);
    const hasOffscreen = typeof OffscreenCanvas !== 'undefined';
    const canvas = hasOffscreen ? new OffscreenCanvas(w, h) : document.createElement('canvas');
    canvas.width = w;
    canvas.height = h;
    const ctx = canvas.getContext('2d');
    if (!ctx)
        throw new Error('Canvas 2D context unavailable');
    ctx.putImageData(imageData, 0, 0);
    const hasHTMLCanvas = typeof globalThis.HTMLCanvasElement !== 'undefined';
    if (hasHTMLCanvas && canvas instanceof globalThis.HTMLCanvasElement) {
        return await new Promise((resolve) => canvas.toBlob((b) => resolve(b), 'image/png'));
    }
    return await canvas.convertToBlob({ type: 'image/png' });
}
let _tokInstance = null;
async function getTokenizer() {
    if (_tokInstance)
        return (text, opts) => _tokInstance(text, opts);
    // Prefer a global AutoTokenizer (if host app preloaded it), else dynamic import.
    const g = globalThis;
    if (g.AutoTokenizer && typeof g.AutoTokenizer.from_pretrained === 'function') {
        // Defensive: ensure proper configuration for global transformers.js
        if (g.env) {
            g.env.allowLocalModels = false;
            g.env.allowRemoteModels = true;
            g.env.remoteHost = 'https://huggingface.co/';
            g.env.remotePathTemplate = '{model}/resolve/{revision}/';
        }
        _tokInstance = await g.AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');
        _tokInstance.pad_token_id = 0;
        return (text, opts) => _tokInstance(text, opts);
    }
    let AutoTokenizerMod = null;
    let env = null;
    try {
        const mod = await import('@xenova/transformers');
        AutoTokenizerMod = mod.AutoTokenizer;
        env = mod.env;
    }
    catch {
        try {
            const spec = '@huggingface/transformers';
            const mod2 = await import(/* @vite-ignore */ spec);
            AutoTokenizerMod = mod2.AutoTokenizer;
            env = mod2.env;
        }
        catch {
            throw new Error('Failed to load a tokenizer. Install @xenova/transformers or provide tokenizerProvider in loadModel options.');
        }
    }
    // Defensive: configure env if available from dynamic import
    if (env) {
        env.allowLocalModels = false;
        env.allowRemoteModels = true;
        env.remoteHost = 'https://huggingface.co/';
        env.remotePathTemplate = '{model}/resolve/{revision}/';
    }
    // Load tokenizer with explicit options to force remote loading
    _tokInstance = await AutoTokenizerMod.from_pretrained('Xenova/clip-vit-base-patch16', {
        local_files_only: false,
        revision: 'main'
    });
    _tokInstance.pad_token_id = 0;
    return (text, opts) => _tokInstance(text, opts);
}
